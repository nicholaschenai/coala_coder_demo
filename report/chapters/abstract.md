# Abstract

This repo presents an implementation of CoALA (Cognitive Architectures for Language Agents) for coding tasks, investigating how cognitive memory systems enhance large language model performance in code generation and problem-solving. We developed a coding agent incorporating episodic, semantic, and procedural memory systems, training on APPS intermediate problems and evaluating on MBPP Plus across GPT-4o 0806 and GPT-4o-mini models against zero-shot, ReAct, and RAG baselines. Results revealed critical model size dependencies: larger models (GPT-4o) achieved modest improvements with memory augmentation (76.5% ReAct vs 77.5% RAG, 77.0% CoALA), while smaller models (GPT-4o-mini) suffered performance degradation (74.9% ReAct vs 72.8% RAG, 73.8% CoALA). Detailed analysis showed smaller models are over-sensitive to memory cues, often leading to suboptimal algorithmic choices, while larger models can effectively leverage relevant memories for optimization insights and edge case handling. We identified significant memory interference effects where irrelevant memories unpredictably influenced agent behavior, with substantial problem state changes (Passâ†”Fail) relative to net performance differences, suggesting much observed variation stems from interference rather than genuine reasoning improvements. Key contributions include: (1) systematic CoALA implementation with comprehensive memory systems for coding tasks, (2) empirical demonstration of model size dependencies in memory utilization effectiveness, (3) characterization of memory interference patterns affecting agent performance, and (4) identification of benchmark limitations that confound evaluation. These findings reveal both promise and challenges in applying cognitive principles to AI coding assistants, suggesting successful memory augmentation requires careful consideration of model capacity, sophisticated retrieval mechanisms, and robust memory quality control.
